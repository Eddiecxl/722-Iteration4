{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3ad19a",
   "metadata": {},
   "source": [
    "# Part 3 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426ae692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/23 11:46:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark in Jupyter\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b311b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Dataset/dementia_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ffbbab",
   "metadata": {},
   "source": [
    "# 3.1 Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357c187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to store the column names with missing values\n",
    "columns_with_missing = []\n",
    "\n",
    "# Loop through each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Count the number of null values, empty strings, \"Unknown\", and \"None\" for the column\n",
    "    count_missing = df.filter(\n",
    "        (col(column).isNull()) |\n",
    "        (col(column) == \"\") |\n",
    "        (col(column) == \"Unknown\") |\n",
    "        (col(column) == \"None\")\n",
    "    ).count()\n",
    "    \n",
    "    # If there are any missing values, add the column name to the list\n",
    "    if count_missing > 0:\n",
    "        columns_with_missing.append(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0156abf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Missing Values\n",
      "----------------------------\n",
      "Prescription\n",
      "Dosage in mg\n",
      "Education_Level\n",
      "Chronic_Health_Conditions\n"
     ]
    }
   ],
   "source": [
    "# Display the header of the table\n",
    "print(\"Columns with Missing Values\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# Display the column names with missing values\n",
    "for column in columns_with_missing:\n",
    "    print(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c92221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# we use Z-score to find outliers and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c8ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers_iqr(df):\n",
    "    # Define an empty dictionary to store the results\n",
    "    results = {}\n",
    "    \n",
    "    # Loop through each numerical column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        if df.select(column).dtypes[0][1] in ['int', 'bigint', 'float', 'double']:\n",
    "            # Calculate quartiles for the column\n",
    "            quartiles = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "            q1 = quartiles[0]\n",
    "            q3 = quartiles[1]\n",
    "            \n",
    "            # Calculate IQR for the column\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            # Calculate lower and upper bounds for outliers\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Count the number of values less than the lower bound or greater than the upper bound\n",
    "            count_outliers = df.filter((col(column) < lower_bound) | (col(column) > upper_bound)).count()\n",
    "            \n",
    "            # Count the number of values less than the lower bound or greater than the upper bound by 3*IQR\n",
    "            count_extremes = df.filter((col(column) < (q1 - 3 * iqr)) | (col(column) > (q3 + 3 * iqr))).count()\n",
    "            \n",
    "            # Add the counts to the dictionary\n",
    "            results[column] = (count_outliers, count_extremes)\n",
    "    \n",
    "    # Return the dictionary containing counts of outliers and extreme values for each column\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e92f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count outliers and extreme values for each column using IQR method\n",
    "outlier_counts_iqr = count_outliers_iqr(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24ee9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name            | Outliers   | Extreme Values\n",
      "-------------------------------------------------\n",
      "Diabetic               | 0          | 0            \n",
      "AlcoholLevel           | 0          | 0            \n",
      "HeartRate              | 9          | 6            \n",
      "BloodOxygenLevel       | 0          | 0            \n",
      "BodyTemperature        | 20         | 20           \n",
      "Weight                 | 10         | 8            \n",
      "MRI_Delay              | 0          | 0            \n",
      "Dosage in mg           | 0          | 0            \n",
      "Age                    | 11         | 6            \n",
      "Cognitive_Test_Scores  | 0          | 0            \n",
      "Dementia               | 0          | 0            \n"
     ]
    }
   ],
   "source": [
    "# Display the header of the table\n",
    "print(\"Column Name            | Outliers   | Extreme Values\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "# Display the counts of outliers and extreme values for each column using IQR method\n",
    "for column, (outliers, extremes) in outlier_counts_iqr.items():\n",
    "    print(f\"{column: <22} | {outliers: <10} | {extremes: <13}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38925ffa",
   "metadata": {},
   "source": [
    "# 3.2 Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0857bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = [\n",
    "    'Dominant_Hand',\n",
    "    'Medication_History',\n",
    "    'Prescription',\n",
    "    'Dosage in mg',\n",
    "    'BloodOxygenLevel'\n",
    "]\n",
    "\n",
    "# Create a new DataFrame by dropping the specified columns\n",
    "df_new = df.drop(*columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1109014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 1000, Total Columns: 24\n",
      "Total Rows after dropped: 1000, Total Columns after dropped: 19\n"
     ]
    }
   ],
   "source": [
    "# Display the total number of rows and columns of the new DataFrame\n",
    "total_rows = df.count()\n",
    "total_columns = len(df.columns)\n",
    "\n",
    "print(f\"Total Rows: {total_rows}, Total Columns: {total_columns}\")\n",
    "\n",
    "# After dropped\n",
    "# Display the total number of rows and columns of the new DataFrame\n",
    "total_rows_new = df_new.count()\n",
    "total_columns_new = len(df_new.columns)\n",
    "\n",
    "print(f\"Total Rows after dropped: {total_rows_new}, Total Columns after dropped: {total_columns_new}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f388aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_info(df):\n",
    "    # Get the schema of the DataFrame\n",
    "    schema = df.schema\n",
    "    \n",
    "    # Create a list to hold column information\n",
    "    columns_info = []\n",
    "    \n",
    "    # Iterate through the schema to get column information\n",
    "    for field in schema:\n",
    "        column_name = field.name\n",
    "        column_type = field.dataType.simpleString()\n",
    "        \n",
    "        # Count non-null values\n",
    "        non_null_count = df.filter((col(column_name).isNotNull()) & \n",
    "                                   (~col(column_name).isin(\"Unknown\", \"None\")) & \n",
    "                                   (~isnan(col(column_name)))).count()\n",
    "        \n",
    "        # Count null values\n",
    "        null_count = df.filter((col(column_name).isNull()) | \n",
    "                               (col(column_name).isin(\"Unknown\", \"None\")) | \n",
    "                               isnan(col(column_name))).count()\n",
    "        \n",
    "        columns_info.append({\n",
    "            \"column_name\": column_name,\n",
    "            \"column_type\": column_type,\n",
    "            \"non_null_count\": non_null_count,\n",
    "            \"null_count\": null_count\n",
    "        })\n",
    "    \n",
    "    summary = {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"total_columns\": total_columns,\n",
    "        \"columns_info\": columns_info\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Call the function to describe the DataFrame\n",
    "summary = spark_info(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6703d7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema and Summary:\n",
      "Column                    Non-Null Count  Null Count Dtype     \n",
      "------------------------------------------------------------\n",
      "Diabetic                  1000            0          int       \n",
      "AlcoholLevel              1000            0          double    \n",
      "HeartRate                 1000            0          int       \n",
      "BodyTemperature           1000            0          double    \n",
      "Weight                    1000            0          double    \n",
      "MRI_Delay                 1000            0          double    \n",
      "Age                       1000            0          int       \n",
      "Education_Level           845             155        string    \n",
      "Gender                    1000            0          string    \n",
      "Family_History            1000            0          string    \n",
      "Smoking_Status            1000            0          string    \n",
      "APOE_ε4                   1000            0          string    \n",
      "Physical_Activity         1000            0          string    \n",
      "Depression_Status         1000            0          string    \n",
      "Cognitive_Test_Scores     1000            0          int       \n",
      "Nutrition_Diet            1000            0          string    \n",
      "Sleep_Quality             1000            0          string    \n",
      "Chronic_Health_Conditions 821             179        string    \n",
      "Dementia                  1000            0          int       \n"
     ]
    }
   ],
   "source": [
    "columns_info = summary['columns_info']\n",
    "\n",
    "print(\"DataFrame Schema and Summary:\")\n",
    "print(f\"{'Column':<25} {'Non-Null Count':<15} {'Null Count':<10} {'Dtype':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for column_info in columns_info:\n",
    "    print(f\"{column_info['column_name']:<25} {column_info['non_null_count']:<15} {column_info['null_count']:<10} {column_info['column_type']:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faba72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Replace null values in \"Education_Level\" column with \"Unknown\"\n",
    "df_new_edu_1 = df_new.withColumn(\"Education_Level\", \n",
    "                                  when(col(\"Education_Level\").isNull() | \n",
    "                                       col(\"Education_Level\").isin(\"None\"),\n",
    "                                       \"Unknown\").otherwise(col(\"Education_Level\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1a872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Count for 'Education_Level' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the null count for \"Education_Level\" column\n",
    "edu_null_count = df_new_edu_1.filter(col(\"Education_Level\").isNull()).count()\n",
    "print(f\"Null Count for 'Education_Level' column: {edu_null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc939cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in \"Chronic_Health_Conditions\" column with \"None\"\n",
    "df_new_edu_1 = df_new_edu_1.withColumn(\"Chronic_Health_Conditions\", \n",
    "                                        when(col(\"Chronic_Health_Conditions\").isNull() | \n",
    "                                             col(\"Chronic_Health_Conditions\").isin(\"Unknown\"),\n",
    "                                             \"None\").otherwise(col(\"Chronic_Health_Conditions\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "357d5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Count for 'Chronic_Health_Conditions' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the null count for \"Chronic_Health_Conditions\" column\n",
    "chronic_health_null_count = df_new_edu_1.filter(col(\"Chronic_Health_Conditions\").isNull()).count()\n",
    "print(f\"Null Count for 'Chronic_Health_Conditions' column: {chronic_health_null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a45ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, when\n",
    "\n",
    "# Define probabilities for each education level\n",
    "primary_prob = 0.4\n",
    "secondary_prob = 0.4\n",
    "diploma_degree_prob = 0.2\n",
    "\n",
    "# Randomly assign education levels based on probabilities\n",
    "df_new_edu_2 = df_new.withColumn(\"Education_Level\", \n",
    "                                 when(rand() <= primary_prob, \"Primary School\")\n",
    "                                .when(rand() <= primary_prob + secondary_prob, \"Secondary School\")\n",
    "                                .otherwise(\"Diploma/Degree\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5af483c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Count for 'Education_Level' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the null count for \"Education_Level\" column\n",
    "edu_null_count2 = df_new_edu_2.filter(col(\"Education_Level\").isNull()).count()\n",
    "print(f\"Null Count for 'Education_Level' column: {edu_null_count2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f9997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values and \"None\" in \"Chronic_Health_Conditions\" column with \"Not healthy\"\n",
    "df_new_edu_2 = df_new_edu_2.withColumn(\"Chronic_Health_Conditions\", \n",
    "                                        when((col(\"Chronic_Health_Conditions\").isNull()) | \n",
    "                                             (col(\"Chronic_Health_Conditions\") == \"None\") | \n",
    "                                             (col(\"Chronic_Health_Conditions\") == \"Unknown\"),\n",
    "                                             \"Not healthy\").otherwise(col(\"Chronic_Health_Conditions\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7ccc567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Count for 'Chronic_Health_Conditions' column: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the null count for \"Chronic_Health_Conditions\" column\n",
    "chronic_health_null_count_2 = df_new_edu_2.filter(col(\"Chronic_Health_Conditions\").isNull()).count()\n",
    "print(f\"Null Count for 'Chronic_Health_Conditions' column: {chronic_health_null_count_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87a5c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for 'Education_Level' column in df_new_edu_1:\n",
      "+----------------+-----+\n",
      "| Education_Level|count|\n",
      "+----------------+-----+\n",
      "|Secondary School|  304|\n",
      "|  Primary School|  389|\n",
      "|         Unknown|  155|\n",
      "|  Diploma/Degree|  152|\n",
      "+----------------+-----+\n",
      "\n",
      "Counts for 'Chronic_Health_Conditions' column in df_new_edu_1:\n",
      "+-------------------------+-----+\n",
      "|Chronic_Health_Conditions|count|\n",
      "+-------------------------+-----+\n",
      "|                     None|  179|\n",
      "|            Heart Disease|  155|\n",
      "|                 Diabetes|  513|\n",
      "|             Hypertension|  153|\n",
      "+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display counts for \"Education_Level\" column in df_new_edu_1\n",
    "print(\"Counts for 'Education_Level' column in df_new_edu_1:\")\n",
    "df_new_edu_1.groupBy(\"Education_Level\").count().show()\n",
    "\n",
    "# Display counts for \"Chronic_Health_Conditions\" column in df_new_edu_1\n",
    "print(\"Counts for 'Chronic_Health_Conditions' column in df_new_edu_1:\")\n",
    "df_new_edu_1.groupBy(\"Chronic_Health_Conditions\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81706b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for 'Education_Level' column in df_new_edu_2:\n",
      "+----------------+-----+\n",
      "| Education_Level|count|\n",
      "+----------------+-----+\n",
      "|Secondary School|  476|\n",
      "|  Primary School|  396|\n",
      "|  Diploma/Degree|  128|\n",
      "+----------------+-----+\n",
      "\n",
      "Counts for 'Chronic_Health_Conditions' column in df_new_edu_2:\n",
      "+-------------------------+-----+\n",
      "|Chronic_Health_Conditions|count|\n",
      "+-------------------------+-----+\n",
      "|              Not healthy|  179|\n",
      "|            Heart Disease|  155|\n",
      "|                 Diabetes|  513|\n",
      "|             Hypertension|  153|\n",
      "+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display counts for \"Education_Level\" column in df_new_edu_2\n",
    "print(\"Counts for 'Education_Level' column in df_new_edu_2:\")\n",
    "df_new_edu_2.groupBy(\"Education_Level\").count().show()\n",
    "\n",
    "# Display counts for \"Chronic_Health_Conditions\" column in df_new_edu_2\n",
    "print(\"Counts for 'Chronic_Health_Conditions' column in df_new_edu_2:\")\n",
    "df_new_edu_2.groupBy(\"Chronic_Health_Conditions\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "234f9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame df_new_2\n",
    "df_new_2 = df_new_edu_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc0e6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Summary:\n",
      "Total Rows     : 1000\n",
      "Total Columns  : 19\n",
      "\n",
      "DataFrame Schema:\n",
      "Column                    Non-Null Count  Null Count Dtype     \n",
      "------------------------------------------------------------\n",
      "Diabetic                  1000            0          int       \n",
      "AlcoholLevel              1000            0          double    \n",
      "HeartRate                 1000            0          int       \n",
      "BodyTemperature           1000            0          double    \n",
      "Weight                    1000            0          double    \n",
      "MRI_Delay                 1000            0          double    \n",
      "Age                       1000            0          int       \n",
      "Education_Level           1000            0          string    \n",
      "Gender                    1000            0          string    \n",
      "Family_History            1000            0          string    \n",
      "Smoking_Status            1000            0          string    \n",
      "APOE_ε4                   1000            0          string    \n",
      "Physical_Activity         1000            0          string    \n",
      "Depression_Status         1000            0          string    \n",
      "Cognitive_Test_Scores     1000            0          int       \n",
      "Nutrition_Diet            1000            0          string    \n",
      "Sleep_Quality             1000            0          string    \n",
      "Chronic_Health_Conditions 1000            0          string    \n",
      "Dementia                  1000            0          int       \n"
     ]
    }
   ],
   "source": [
    "def spark_info(df):\n",
    "    # Get the schema of the DataFrame\n",
    "    schema = df.schema\n",
    "    \n",
    "    # Create a list to hold column information\n",
    "    columns_info = []\n",
    "    \n",
    "    # Iterate through the schema to get column information\n",
    "    for field in schema:\n",
    "        column_name = field.name\n",
    "        column_type = field.dataType.simpleString()\n",
    "        \n",
    "        # Count non-null values\n",
    "        non_null_count = df_new_2.filter(col(column_name).isNotNull()).count()\n",
    "        \n",
    "        # Count null values\n",
    "        null_count = df_new_2.filter(col(column_name).isNull() | isnan(col(column_name))).count()\n",
    "        \n",
    "        columns_info.append((column_name, column_type, non_null_count, null_count))\n",
    "    \n",
    "    # Display the DataFrame schema and summary\n",
    "    total_rows = df_new_2.count()\n",
    "    total_columns = len(schema)\n",
    "    \n",
    "    # Print the summary table\n",
    "    print(f\"DataFrame Summary:\")\n",
    "    print(f\"{'Total Rows':<15}: {total_rows}\")\n",
    "    print(f\"{'Total Columns':<15}: {total_columns}\")\n",
    "    print(\"\\nDataFrame Schema:\")\n",
    "    print(f\"{'Column':<25} {'Non-Null Count':<15} {'Null Count':<10} {'Dtype':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for column_info in columns_info:\n",
    "        print(f\"{column_info[0]:<25} {column_info[2]:<15} {column_info[3]:<10} {column_info[1]:<10}\")\n",
    "\n",
    "# Call the function to describe the DataFrame\n",
    "spark_info(df_new_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd3b10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# we use Z-score to find outliers and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1f91ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name            | Outliers   | Extreme Values\n",
      "-------------------------------------------------\n",
      "Diabetic               | 0          | 0            \n",
      "AlcoholLevel           | 0          | 0            \n",
      "HeartRate              | 9          | 6            \n",
      "BodyTemperature        | 20         | 20           \n",
      "Weight                 | 10         | 8            \n",
      "MRI_Delay              | 0          | 0            \n",
      "Age                    | 11         | 6            \n",
      "Cognitive_Test_Scores  | 0          | 0            \n",
      "Dementia               | 0          | 0            \n"
     ]
    }
   ],
   "source": [
    "def count_outliers_iqr(df):\n",
    "    # Define an empty dictionary to store the results\n",
    "    results = {}\n",
    "    \n",
    "    # Loop through each numerical column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        if df.select(column).dtypes[0][1] in ['int', 'bigint', 'float', 'double']:\n",
    "            # Calculate quartiles for the column\n",
    "            quartiles = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "            q1 = quartiles[0]\n",
    "            q3 = quartiles[1]\n",
    "            \n",
    "            # Calculate IQR for the column\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            # Calculate lower and upper bounds for outliers\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Count the number of values less than the lower bound or greater than the upper bound\n",
    "            count_outliers = df.filter((col(column) < lower_bound) | (col(column) > upper_bound)).count()\n",
    "            \n",
    "            # Count the number of values less than the lower bound or greater than the upper bound by 3*IQR\n",
    "            count_extremes = df.filter((col(column) < (q1 - 3 * iqr)) | (col(column) > (q3 + 3 * iqr))).count()\n",
    "            \n",
    "            # Add the counts to the dictionary\n",
    "            results[column] = (count_outliers, count_extremes)\n",
    "    \n",
    "    # Return the dictionary containing counts of outliers and extreme values for each column\n",
    "    return results\n",
    "\n",
    "# Count outliers and extreme values for each column using IQR method for df_new_2\n",
    "outlier_counts_iqr_new = count_outliers_iqr(df_new_2)\n",
    "\n",
    "# Display the header of the table\n",
    "print(\"Column Name            | Outliers   | Extreme Values\")\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "# Display the counts of outliers and extreme values for each column using IQR method for df_new_2\n",
    "for column, (outliers, extremes) in outlier_counts_iqr_new.items():\n",
    "    print(f\"{column: <22} | {outliers: <10} | {extremes: <13}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4653c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the values of outliers and extreme values\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def find_outliers_iqr(df):\n",
    "    # Define an empty dictionary to store the results\n",
    "    results = {}\n",
    "    \n",
    "    # Loop through each numerical column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        if df.select(column).dtypes[0][1] in ['int', 'bigint', 'float', 'double']:\n",
    "            # Calculate quartiles for the column\n",
    "            quartiles = df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "            q1 = quartiles[0]\n",
    "            q3 = quartiles[1]\n",
    "            \n",
    "            # Calculate IQR for the column\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            # Calculate lower and upper bounds for outliers\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            # Find outliers and extreme values\n",
    "            outliers = df.filter((col(column) < lower_bound) | (col(column) > upper_bound)).select(column).collect()\n",
    "            extremes = df.filter((col(column) < (q1 - 3 * iqr)) | (col(column) > (q3 + 3 * iqr))).select(column).collect()\n",
    "            \n",
    "            # If there are outliers or extreme values, add them to the results dictionary\n",
    "            if outliers or extremes:\n",
    "                results[column] = {\n",
    "                    \"outliers\": [row[column] for row in outliers],\n",
    "                    \"extremes\": [row[column] for row in extremes]\n",
    "                }\n",
    "    \n",
    "    # Return the dictionary containing outliers and extreme values for each column\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d9d2e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: HeartRate\n",
      "Outliers:\n",
      "[194, 20, 182, 150, 190, 210, 320, 32, 1]\n",
      "Extremes:\n",
      "[194, 182, 190, 210, 320, 1]\n",
      "Column: BodyTemperature\n",
      "Outliers:\n",
      "[41.34388999, 40.27682345, 42.25475742, 316.9464443, 32.10684275, 31.39061679, 30.94032093, 46.93897455, 42.41509512, 42.62742142, 46.42802935, 356.760164, 42.12994276, 43.97082575, 41.56382146, 45.2072102, 29.81030025, 30.06031921, 51.16408019, 22.06352268]\n",
      "Extremes:\n",
      "[41.34388999, 40.27682345, 42.25475742, 316.9464443, 32.10684275, 31.39061679, 30.94032093, 46.93897455, 42.41509512, 42.62742142, 46.42802935, 356.760164, 42.12994276, 43.97082575, 41.56382146, 45.2072102, 29.81030025, 30.06031921, 51.16408019, 22.06352268]\n",
      "Column: Weight\n",
      "Outliers:\n",
      "[717.5014573, 519.5217171, 161.4996653, 287.3927216, 128.5190244, 540.7445893, 911.6282937, 796.9402277, 631.0099286, 200.3169645]\n",
      "Extremes:\n",
      "[717.5014573, 519.5217171, 287.3927216, 540.7445893, 911.6282937, 796.9402277, 631.0099286, 200.3169645]\n",
      "Column: Age\n",
      "Outliers:\n",
      "[120, 200, 157, 111, 234, 113, 112, 114, 178, 251, 244]\n",
      "Extremes:\n",
      "[200, 157, 234, 178, 251, 244]\n"
     ]
    }
   ],
   "source": [
    "# Call the function to find outliers and extreme values for df_new_2\n",
    "outlier_values_iqr = find_outliers_iqr(df_new_2)\n",
    "\n",
    "# Display the outliers and extreme values\n",
    "for column, values in outlier_values_iqr.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    if \"outliers\" in values:\n",
    "        print(\"Outliers:\")\n",
    "        print(values[\"outliers\"])\n",
    "    if \"extremes\" in values:\n",
    "        print(\"Extremes:\")\n",
    "        print(values[\"extremes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8f9895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Diabetic: integer (nullable = true)\n",
      " |-- AlcoholLevel: double (nullable = true)\n",
      " |-- HeartRate: integer (nullable = true)\n",
      " |-- BodyTemperature: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- MRI_Delay: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Education_Level: string (nullable = false)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Family_History: string (nullable = true)\n",
      " |-- Smoking_Status: string (nullable = true)\n",
      " |-- APOE_ε4: string (nullable = true)\n",
      " |-- Physical_Activity: string (nullable = true)\n",
      " |-- Depression_Status: string (nullable = true)\n",
      " |-- Cognitive_Test_Scores: integer (nullable = true)\n",
      " |-- Nutrition_Diet: string (nullable = true)\n",
      " |-- Sleep_Quality: string (nullable = true)\n",
      " |-- Chronic_Health_Conditions: string (nullable = true)\n",
      " |-- Dementia: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0654582e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ubuntu/722-Iteration4/Dataset/df_new_2.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save DataFrame to a CSV file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_new_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset/df_new_2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:955\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    949\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m    950\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    953\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m    954\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ubuntu/722-Iteration4/Dataset/df_new_2.csv already exists."
     ]
    }
   ],
   "source": [
    "# Save DataFrame to a CSV file\n",
    "#df_new_2.write.csv(\"Dataset/df_new_2.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137243d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue in Part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
