{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e256844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark in Jupyter\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc50116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One with dementia one without dementia\n",
    "df = spark.read.csv('Dataset/df_new_6d.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('Dataset/df_new_6wd.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9e068",
   "metadata": {},
   "source": [
    "# Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cbff0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Association Rules Mining\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming your DataFrame is named 'df' and contains transaction data\n",
    "\n",
    "# Convert string columns to numeric using StringIndexer\n",
    "string_cols = ['Education_Level', 'Family_History', 'Smoking_Status', 'APOE_ε4', \n",
    "               'Depression_Status', 'Education_Group']\n",
    "\n",
    "# Initialize StringIndexer objects\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"skip\") for col in string_cols]\n",
    "\n",
    "# Apply transformations\n",
    "for indexer in indexers:\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Convert indexed columns to one-hot encoded vectors\n",
    "encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers], \n",
    "                        outputCols=[col+\"_encoded\" for col in string_cols])\n",
    "\n",
    "# Apply one-hot encoding\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "# Define the columns you want to assemble into an array\n",
    "input_cols = ['AlcoholLevel', 'HeartRate', 'BodyTemperature', 'Weight', 'MRI_Delay', 'Age', \n",
    "              'Education_Level_encoded', 'Family_History_encoded', 'Smoking_Status_encoded', \n",
    "              'APOE_ε4_encoded', 'Depression_Status_encoded', 'Cognitive_Test_Scores', 'Education_Group_encoded','Dementia']\n",
    "\n",
    "# Initialize the VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform the DataFrame to add the features as an array column\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Split the 'Dementia' column into an array of items\n",
    "df = df.withColumn(\"Dementia_array\", split(df[\"Dementia\"], \",\").cast(ArrayType(StringType())))\n",
    "\n",
    "# Select only the 'Dementia_array' and 'features' columns\n",
    "df = df.select(\"Dementia_array\", \"features\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Now, let's perform the FP-growth model\n",
    "\n",
    "# Assuming 'Dementia_array' is the column containing the items in each transaction\n",
    "fp_growth = FPGrowth(itemsCol=\"Dementia_array\", minSupport=0.0, minConfidence=0.0)\n",
    "\n",
    "# Train the FP-growth model\n",
    "model = fp_growth.fit(df)\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Extract association rules from the model\n",
    "association_rules = model.associationRules\n",
    "\n",
    "# Display generated association rules\n",
    "print(\"Association Rules:\")\n",
    "model.associationRules.show()\n",
    "\n",
    "# Transform examines the input items against all the association rules and summarizes the consequents as prediction\n",
    "print(\"Transformed DataFrame:\")\n",
    "transformed_df = model.transform(df)\n",
    "transformed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5883014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, expr\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PrefixSpan\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load your DataFrame\n",
    "df = spark.read.csv('Dataset/df_new_6d.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema to understand the available columns\n",
    "df.printSchema()\n",
    "\n",
    "# Assuming 'Dementia' is a column that contains sequences in a string format\n",
    "# First, let's transform this column into an array of strings\n",
    "df = df.withColumn(\"Dementia_array\", split(col(\"Dementia\"), \",\"))\n",
    "\n",
    "# Transform each string in the array into an array containing that string\n",
    "df = df.withColumn(\"Dementia_array_of_arrays\", expr(\"transform(Dementia_array, x -> array(x))\"))\n",
    "\n",
    "# PrefixSpan expects a DataFrame with a single column 'sequence' containing the sequences\n",
    "df = df.select(col(\"Dementia_array_of_arrays\").alias(\"sequence\"))\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Apply PrefixSpan\n",
    "prefixspan = PrefixSpan(minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000)\n",
    "\n",
    "# Train the model\n",
    "model = prefixspan.findFrequentSequentialPatterns(df)\n",
    "\n",
    "# Show the frequent sequential patterns\n",
    "print(\"Frequent Sequential Patterns:\")\n",
    "model.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9090b0",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390dac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart df\n",
    "df = spark.read.csv('Dataset/df_new_6d.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df\n",
    "def spark_info(df):\n",
    "    # Get the schema of the DataFrame\n",
    "    schema = df.schema\n",
    "    \n",
    "    # Create a list to hold column information\n",
    "    columns_info = []\n",
    "    \n",
    "    # Iterate through the schema to get column information\n",
    "    for field in schema:\n",
    "        column_name = field.name\n",
    "        column_type = field.dataType.simpleString()\n",
    "        \n",
    "        # Count non-null values\n",
    "        non_null_count = df.filter(col(column_name).isNotNull()).count()\n",
    "        \n",
    "        # Count null values\n",
    "        null_count = df.filter(col(column_name).isNull() | isnan(col(column_name))).count()\n",
    "        \n",
    "        columns_info.append((column_name, column_type, non_null_count, null_count))\n",
    "    \n",
    "    # Display the DataFrame schema and summary\n",
    "    total_rows = df.count()\n",
    "    total_columns = len(schema)\n",
    "    \n",
    "    # Print the summary table\n",
    "    print(f\"DataFrame Summary:\")\n",
    "    print(f\"{'Total Rows':<15}: {total_rows}\")\n",
    "    print(f\"{'Total Columns':<15}: {total_columns}\")\n",
    "    print(\"\\nDataFrame Schema:\")\n",
    "    print(f\"{'Column':<25} {'Non-Null Count':<15} {'Null Count':<10} {'Dtype':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for column_info in columns_info:\n",
    "        print(f\"{column_info[0]:<25} {column_info[2]:<15} {column_info[3]:<10} {column_info[1]:<10}\")\n",
    "\n",
    "# Call the function to describe the DataFrame\n",
    "spark_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a70283",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogisticRegressionClassification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('Dataset/df_new_6d.csv', header=True, inferSchema=True)\n",
    "\n",
    "# We excluded Cognitive Test Score\n",
    "# Select all features and the target variable\n",
    "categorical_features = ['Family_History', 'Smoking_Status', 'APOE_ε4', 'Depression_Status', 'Education_Group']\n",
    "numeric_features = ['Age', 'AlcoholLevel', 'HeartRate', 'BodyTemperature', 'Weight', 'MRI_Delay']\n",
    "target = 'Dementia'\n",
    "\n",
    "# Index categorical features\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_features]\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(inputCols=[column + \"_index\" for column in categorical_features] + numeric_features, outputCol=\"features\")\n",
    "\n",
    "# Initialize the LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=target, featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
    "\n",
    "# Train-Test Split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on both the training and testing data\n",
    "train_predictions = pipeline_model.transform(train_df)\n",
    "test_predictions = pipeline_model.transform(test_df)\n",
    "\n",
    "# Evaluate the Model for both training and testing sets using accuracy metric\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target, metricName=\"accuracy\")\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(f\"Training Set Accuracy (Evaluator): {train_accuracy}\")\n",
    "print(f\"Testing Set Accuracy (Evaluator): {test_accuracy}\")\n",
    "\n",
    "# Calculate correct and incorrect predictions\n",
    "def calculate_correct_wrong(predictions, label_col):\n",
    "    pred_labels = predictions.select('prediction', label_col).rdd\n",
    "    pred_labels = pred_labels.map(lambda row: (row['prediction'], row[label_col]))\n",
    "    \n",
    "    tp = pred_labels.filter(lambda pl: pl[0] == 1.0 and pl[1] == 1.0).count()\n",
    "    tn = pred_labels.filter(lambda pl: pl[0] == 0.0 and pl[1] == 0.0).count()\n",
    "    fp = pred_labels.filter(lambda pl: pl[0] == 1.0 and pl[1] == 0.0).count()\n",
    "    fn = pred_labels.filter(lambda pl: pl[0] == 0.0 and pl[1] == 1.0).count()\n",
    "    \n",
    "    correct = tp + tn\n",
    "    wrong = fp + fn\n",
    "    \n",
    "    total = correct + wrong\n",
    "    correct_pct = (correct / total) * 100\n",
    "    wrong_pct = (wrong / total) * 100\n",
    "    \n",
    "    return correct, wrong, correct_pct, wrong_pct\n",
    "\n",
    "train_correct, train_wrong, train_correct_pct, train_wrong_pct = calculate_correct_wrong(train_predictions, target)\n",
    "test_correct, test_wrong, test_correct_pct, test_wrong_pct = calculate_correct_wrong(test_predictions, target)\n",
    "\n",
    "print(f\"Training Set Correct: {train_correct}\")\n",
    "print(f\"Training Set Wrong: {train_wrong}\")\n",
    "print(f\"Training Set Correct (%): {train_correct_pct}\")\n",
    "print(f\"Training Set Wrong (%): {train_wrong_pct}\")\n",
    "print(f\"Testing Set Correct: {test_correct}\")\n",
    "print(f\"Testing Set Wrong: {test_wrong}\")\n",
    "print(f\"Testing Set Correct (%): {test_correct_pct}\")\n",
    "print(f\"Testing Set Wrong (%): {test_wrong_pct}\")\n",
    "\n",
    "# Define function to plot logistic regression results for both training and testing sets\n",
    "def plot_logistic_regression_results(train_correct, train_correct_pct, train_wrong, train_wrong_pct,\n",
    "                                     test_correct, test_correct_pct, test_wrong, test_wrong_pct):\n",
    "    labels = ['Training Set', 'Testing Set']\n",
    "    correct = [train_correct, test_correct]\n",
    "    correct_pct = [round(train_correct_pct, 2), round(test_correct_pct, 2)]\n",
    "    wrong = [train_wrong, test_wrong]\n",
    "    wrong_pct = [round(train_wrong_pct, 2), round(test_wrong_pct, 2)]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width, correct, width, label='Correct', color='lightgreen')\n",
    "    rects2 = ax.bar(x, wrong, width, label='Wrong', color='salmon')\n",
    "    rects3 = ax.bar(x + width, correct_pct, width, label='Correct (%)', color='skyblue')\n",
    "    rects4 = ax.bar(x + 2*width, wrong_pct, width, label='Wrong (%)', color='orange')\n",
    "    ax.set_ylabel('Count / Percentage')\n",
    "    ax.set_title('Logistic Regression Model Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend(loc='center')\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  \n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "    autolabel(rects4)\n",
    "    plt.show()\n",
    "\n",
    "# Plot logistic regression results for both training and testing sets\n",
    "plot_logistic_regression_results(train_correct, train_correct_pct, train_wrong, train_wrong_pct,\n",
    "                                 test_correct, test_correct_pct, test_wrong, test_wrong_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbcdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_accuracy(train_accuracy, test_accuracy):\n",
    "    # Generate x values for interpolation\n",
    "    x_values = np.linspace(0, 1, num=100)\n",
    "    \n",
    "    # Interpolate between the two accuracy points\n",
    "    train_line = np.linspace(0, train_accuracy, num=100)\n",
    "    test_line = np.linspace(0, test_accuracy, num=100)\n",
    "    \n",
    "    # Plot training set accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_values, train_line, label='Training Set Accuracy')\n",
    "    plt.plot(x_values, test_line, label='Testing Set Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Logistic Regression Model Evaluation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot accuracy for both training and testing sets\n",
    "plot_accuracy(train_accuracy, test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f79fed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DecisionTreeClassification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('Dataset/df_new_6d.csv', header=True, inferSchema=True)\n",
    "# Drop the \"Cognitive_Test_Score\" column from the DataFrame\n",
    "df = df.drop(\"Cognitive_Test_Score\")\n",
    "\n",
    "# We excluded Cognitive Test Score\n",
    "# Select all features and the target variable\n",
    "categorical_features = ['Family_History', 'Smoking_Status', 'APOE_ε4', 'Depression_Status', 'Education_Group']\n",
    "numeric_features = ['Age', 'AlcoholLevel', 'HeartRate', 'BodyTemperature', 'Weight', 'MRI_Delay']\n",
    "target = 'Dementia'\n",
    "\n",
    "# Index categorical features\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_features]\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(inputCols=[column + \"_index\" for column in categorical_features] + numeric_features, outputCol=\"features\")\n",
    "\n",
    "# Initialize the DecisionTreeClassifier model\n",
    "dt = DecisionTreeClassifier(labelCol=target, featuresCol=\"features\")\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline_dt = Pipeline(stages=indexers + [assembler, dt])\n",
    "\n",
    "# Train-Test Split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "pipeline_model_dt = pipeline_dt.fit(train_df)\n",
    "\n",
    "# Make predictions on both the training and testing data\n",
    "train_predictions_dt = pipeline_model_dt.transform(train_df)\n",
    "test_predictions_dt = pipeline_model_dt.transform(test_df)\n",
    "\n",
    "# Evaluate the Model for both training and testing sets using accuracy metric\n",
    "evaluator_dt = MulticlassClassificationEvaluator(labelCol=target, metricName=\"accuracy\")\n",
    "train_accuracy_dt = evaluator_dt.evaluate(train_predictions_dt)\n",
    "test_accuracy_dt = evaluator_dt.evaluate(test_predictions_dt)\n",
    "print(f\"Decision Tree - Training Set Accuracy (Evaluator): {train_accuracy_dt}\")\n",
    "print(f\"Decision Tree - Testing Set Accuracy (Evaluator): {test_accuracy_dt}\")\n",
    "\n",
    "# Calculate correct and incorrect predictions\n",
    "def calculate_correct_wrong(predictions, label_col):\n",
    "    pred_labels = predictions.select('prediction', label_col).rdd\n",
    "    pred_labels = pred_labels.map(lambda row: (row['prediction'], row[label_col]))\n",
    "    \n",
    "    tp = pred_labels.filter(lambda pl: pl[0] == 1.0 and pl[1] == 1.0).count()\n",
    "    tn = pred_labels.filter(lambda pl: pl[0] == 0.0 and pl[1] == 0.0).count()\n",
    "    fp = pred_labels.filter(lambda pl: pl[0] == 1.0 and pl[1] == 0.0).count()\n",
    "    fn = pred_labels.filter(lambda pl: pl[0] == 0.0 and pl[1] == 1.0).count()\n",
    "    \n",
    "    correct = tp + tn\n",
    "    wrong = fp + fn\n",
    "    \n",
    "    total = correct + wrong\n",
    "    correct_pct = (correct / total) * 100\n",
    "    wrong_pct = (wrong / total) * 100\n",
    "    \n",
    "    return correct, wrong, correct_pct, wrong_pct\n",
    "\n",
    "train_correct_dt, train_wrong_dt, train_correct_pct_dt, train_wrong_pct_dt = calculate_correct_wrong(train_predictions_dt, target)\n",
    "test_correct_dt, test_wrong_dt, test_correct_pct_dt, test_wrong_pct_dt = calculate_correct_wrong(test_predictions_dt, target)\n",
    "\n",
    "print(f\"Decision Tree - Training Set Correct: {train_correct_dt}\")\n",
    "print(f\"Decision Tree - Training Set Wrong: {train_wrong_dt}\")\n",
    "print(f\"Decision Tree - Training Set Correct (%): {train_correct_pct_dt}\")\n",
    "print(f\"Decision Tree - Training Set Wrong (%): {train_wrong_pct_dt}\")\n",
    "print(f\"Decision Tree - Testing Set Correct: {test_correct_dt}\")\n",
    "print(f\"Decision Tree - Testing Set Wrong: {test_wrong_dt}\")\n",
    "print(f\"Decision Tree - Testing Set Correct (%): {test_correct_pct_dt}\")\n",
    "print(f\"Decision Tree - Testing Set Wrong (%): {test_wrong_pct_dt}\")\n",
    "\n",
    "# Define function to plot decision tree results for both training and testing sets\n",
    "def plot_decision_tree_results(train_correct, train_correct_pct, train_wrong, train_wrong_pct,\n",
    "                               test_correct, test_correct_pct, test_wrong, test_wrong_pct):\n",
    "    labels = ['Training Set', 'Testing Set']\n",
    "    correct = [train_correct, test_correct]\n",
    "    correct_pct = [round(train_correct_pct, 2), round(test_correct_pct, 2)]\n",
    "    wrong = [train_wrong, test_wrong]\n",
    "    wrong_pct = [round(train_wrong_pct, 2), round(test_wrong_pct, 2)]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width, correct, width, label='Correct', color='lightgreen')\n",
    "    rects2 = ax.bar(x, wrong, width, label='Wrong', color='salmon')\n",
    "    rects3 = ax.bar(x + width, correct_pct, width, label='Correct (%)', color='skyblue')\n",
    "    rects4 = ax.bar(x + 2*width, wrong_pct, width, label='Wrong (%)', color='orange')\n",
    "    ax.set_ylabel('Count / Percentage')\n",
    "    ax.set_title('Decision Tree Model Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend(loc='center')\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  \n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "    autolabel(rects4)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision tree results for both training and testing sets\n",
    "plot_decision_tree_results(train_correct_dt, train_correct_pct_dt, train_wrong_dt, train_wrong_pct_dt,\n",
    "                           test_correct_dt, test_correct_pct_dt, test_wrong_dt, test_wrong_pct_dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80932dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_accuracy(train_accuracy, test_accuracy):\n",
    "    # Generate x values for interpolation\n",
    "    x_values = np.linspace(0, 1, num=100)\n",
    "    \n",
    "    # Interpolate between the two accuracy points\n",
    "    train_line = np.linspace(0, train_accuracy, num=100)\n",
    "    test_line = np.linspace(0, test_accuracy, num=100)\n",
    "    \n",
    "    # Plot training set accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_values, train_line, label='Training Set Accuracy')\n",
    "    plt.plot(x_values, test_line, label='Testing Set Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Decision Tree Model Evaluation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot accuracy for both training and testing sets\n",
    "plot_accuracy(train_accuracy_dt, test_accuracy_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b687ade",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline as SparkPipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "#categorical feature causing the issue\n",
    "#categorical_features = ['Dementia','Education_Group']\n",
    "categorical_features = ['Family_History', 'Smoking_Status', 'APOE_ε4', 'Depression_Status', 'Education_Group']\n",
    "\n",
    "# Define preprocessing steps for numerical and categorical features\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "# Create a column transformer to apply different preprocessing steps to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas DataFrame for sklearn\n",
    "X_train = train_df.select(*numeric_features, *categorical_features).toPandas()\n",
    "y_train = train_df.select(target).toPandas()\n",
    "\n",
    "X_test = test_df.select(*numeric_features, *categorical_features).toPandas()\n",
    "y_test = test_df.select(target).toPandas()\n",
    "\n",
    "\n",
    "# Create a pipeline with preprocessing and KNN classifier\n",
    "pipeline_knn = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on both the training and testing data\n",
    "train_predictions_knn = pipeline_knn.predict(X_train)\n",
    "test_predictions_knn = pipeline_knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy using sklearn for both training and testing sets\n",
    "train_accuracy_sklearn_knn = accuracy_score(y_train, train_predictions_knn)\n",
    "test_accuracy_sklearn_knn = accuracy_score(y_test, test_predictions_knn)\n",
    "print(\"KNN - Training Set Accuracy:\", train_accuracy_sklearn_knn)\n",
    "print(\"KNN - Testing Set Accuracy:\", test_accuracy_sklearn_knn)\n",
    "\n",
    "# Calculate confusion matrix using sklearn for both training and testing sets\n",
    "train_cm_knn = confusion_matrix(y_train, train_predictions_knn)\n",
    "test_cm_knn = confusion_matrix(y_test, test_predictions_knn)\n",
    "\n",
    "# Calculate number of correct and wrong predictions for both training and testing sets\n",
    "train_correct_knn = train_cm_knn[0, 0] + train_cm_knn[1, 1]\n",
    "train_wrong_knn = train_cm_knn[0, 1] + train_cm_knn[1, 0]\n",
    "test_correct_knn = test_cm_knn[0, 0] + test_cm_knn[1, 1]\n",
    "test_wrong_knn = test_cm_knn[0, 1] + test_cm_knn[1, 0]\n",
    "\n",
    "# Calculate percentages of correct and wrong predictions for both training and testing sets\n",
    "train_total_knn = len(y_train)\n",
    "test_total_knn = len(y_test)\n",
    "train_correct_pct_knn = (train_correct_knn / train_total_knn) * 100\n",
    "train_wrong_pct_knn = (train_wrong_knn / train_total_knn) * 100\n",
    "test_correct_pct_knn = (test_correct_knn / test_total_knn) * 100\n",
    "test_wrong_pct_knn = (test_wrong_knn / test_total_knn) * 100\n",
    "\n",
    "# Define function to plot KNN results for both training and testing sets\n",
    "def plot_knn_results(train_correct, train_correct_pct, train_wrong, train_wrong_pct,\n",
    "                     test_correct, test_correct_pct, test_wrong, test_wrong_pct):\n",
    "    labels = ['Training Set', 'Testing Set']\n",
    "    correct = [train_correct, test_correct]\n",
    "    correct_pct = [round(train_correct_pct, 2), round(test_correct_pct, 2)]\n",
    "    wrong = [train_wrong, test_wrong]\n",
    "    wrong_pct = [round(train_wrong_pct, 2), round(test_wrong_pct, 2)]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.2\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width, correct, width, label='Correct', color='lightgreen')\n",
    "    rects2 = ax.bar(x, wrong, width, label='Wrong', color='salmon')\n",
    "    rects3 = ax.bar(x + width, correct_pct, width, label='Correct (%)', color='skyblue')\n",
    "    rects4 = ax.bar(x + 2*width, wrong_pct, width, label='Wrong (%)', color='orange')\n",
    "    ax.set_ylabel('Count / Percentage')\n",
    "    ax.set_title('KNN Model Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend(loc='center')\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  \n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "    autolabel(rects4)\n",
    "    plt.show()\n",
    "\n",
    "# Plot KNN results for both training and testing sets\n",
    "plot_knn_results(train_correct_knn, train_correct_pct_knn, train_wrong_knn, train_wrong_pct_knn,\n",
    "                 test_correct_knn, test_correct_pct_knn, test_wrong_knn, test_wrong_pct_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_accuracy_knn(train_accuracy, test_accuracy):\n",
    "    # Generate x values for interpolation\n",
    "    x_values = np.linspace(0, 1, num=100)\n",
    "    \n",
    "    # Interpolate between the two accuracy points\n",
    "    train_line = np.linspace(0, train_accuracy, num=100)\n",
    "    test_line = np.linspace(0, test_accuracy, num=100)\n",
    "    \n",
    "    # Plot training set accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_values, train_line, label='Training Set Accuracy')\n",
    "    plt.plot(x_values, test_line, label='Testing Set Accuracy')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('KNN Model Evaluation')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot accuracy for both training and testing sets\n",
    "plot_accuracy_knn(train_accuracy_sklearn_knn, test_accuracy_sklearn_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
